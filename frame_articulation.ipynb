{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18401,"status":"ok","timestamp":1726369228067,"user":{"displayName":"Rakshitha Rao Ailneni","userId":"10554421985684110379"},"user_tz":300},"id":"iXum43dr2Wzm","outputId":"a51fc41f-b442-42c4-d406-d6878ea8e166"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting OpenAI\n","  Downloading openai-1.45.0-py3-none-any.whl.metadata (22 kB)\n","Requirement already satisfied: tenacity in /usr/local/lib/python3.10/dist-packages (9.0.0)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from OpenAI) (3.7.1)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from OpenAI) (1.7.0)\n","Collecting httpx<1,>=0.23.0 (from OpenAI)\n","  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n","Collecting jiter<1,>=0.4.0 (from OpenAI)\n","  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from OpenAI) (2.9.1)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from OpenAI) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from OpenAI) (4.66.5)\n","Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from OpenAI) (4.12.2)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->OpenAI) (3.8)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->OpenAI) (1.2.2)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->OpenAI) (2024.8.30)\n","Collecting httpcore==1.* (from httpx<1,>=0.23.0->OpenAI)\n","  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n","Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->OpenAI)\n","  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->OpenAI) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.23.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->OpenAI) (2.23.3)\n","Downloading openai-1.45.0-py3-none-any.whl (374 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.1/374.1 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: jiter, h11, httpcore, httpx, OpenAI\n","Successfully installed OpenAI-1.45.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 jiter-0.5.0\n"]}],"source":["!pip install OpenAI tenacity"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":4712,"status":"ok","timestamp":1726369232774,"user":{"displayName":"Rakshitha Rao Ailneni","userId":"10554421985684110379"},"user_tz":300},"id":"OWKNa2OR2Iyq"},"outputs":[],"source":["#another method of calling openai\n","from openai import OpenAI, BadRequestError\n","from openai.types.chat import ChatCompletion\n","from tenacity import retry, stop_after_attempt, wait_random_exponential\n","import time\n","from typing import Optional\n","\n","class MinimumDelay:\n","    def __init__(self, delay: float | int):\n","        self.delay = delay\n","        self.start = None\n","\n","    def __enter__(self):\n","        self.start = time.time()\n","\n","    def __exit__(self, exc_type, exc_val, exc_tb):\n","        end = time.time()\n","        seconds = end - self.start\n","        if self.delay > seconds:\n","            time.sleep(self.delay - seconds)\n","\n","@retry(wait=wait_random_exponential(min=1, max=90), stop=stop_after_attempt(3))\n","def chat(client: OpenAI, delay: float | int, **kwargs) -> ChatCompletion | None:\n","    try:\n","        with MinimumDelay(delay):\n","            return client.chat.completions.create(**kwargs)\n","    except BadRequestError as e:\n","        print(f\"Bad Request: {e}\")\n","        if \"safety\" in e.message:\n","            return None\n","        raise e\n","    except Exception as e:\n","        print(f\"Exception: {e}\")\n","        raise e"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1726369232775,"user":{"displayName":"Rakshitha Rao Ailneni","userId":"10554421985684110379"},"user_tz":300},"id":"dfCipibb2FTb"},"outputs":[],"source":["def read_jsonl(path):\n","    with open(path, \"r\") as f:\n","        for line in f:\n","            line = line.strip()\n","            if line:\n","                ex = json.loads(line)\n","                yield ex\n","\n","def write_jsonl(path, data):\n","    with open(path, \"w\") as f:\n","        for ex in data:\n","            f.write(json.dumps(ex) + \"\\n\")\n","\n","# Function to encode the image\n","def encode_image(image_path):\n","    with open(image_path, \"rb\") as image_file:\n","        return base64.b64encode(image_file.read()).decode(\"utf-8\")"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":1559,"status":"ok","timestamp":1726369234327,"user":{"displayName":"Rakshitha Rao Ailneni","userId":"10554421985684110379"},"user_tz":300},"id":"SF3SVbRi2FTb"},"outputs":[],"source":["import base64\n","import requests\n","import json\n","import os\n","from tqdm import tqdm\n","#define your api_key\n","client = OpenAI(api_key, timeout=90)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8UpQQRUW69My"},"outputs":[],"source":["import pandas as pd\n","\n","def csv_to_jsonl(csv_file, jsonl_file):\n","\n","    df = pd.read_csv(csv_file, sep='\\t')\n","    with open(jsonl_file, 'w') as f:\n","        for index, row in df.iterrows():\n","            f.write(row.to_json() + '\\n')\n","\n","csv_to_jsonl('test/Test.csv', 'test_miso.jsonl')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fP-Pv_A62FTd"},"outputs":[],"source":["preds=[]\n","seen_ids=set()\n","skipped=[]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7171147,"status":"ok","timestamp":1725696588170,"user":{"displayName":"Rakshitha Rao Ailneni","userId":"10554421985684110379"},"user_tz":300},"id":"XllryCwn2FTd","outputId":"4e92d044-2c28-4952-84ed-6969dd808a33"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 1000/1000 [1:59:30<00:00,  7.17s/it]\n"]}],"source":["# Path to your image\n","image_path = 'test/'\n","data_path = 'test_miso.jsonl'\n","demo_image_path = 'TRAINING/'\n","\n","sys = '''You are an expert linguistic assistant.\n","Frames of communication select particular aspects of an issue and make them salient in communicating a message.\n","Frames of communication are ubiquitous in social media discourse and can impact how people understand issues and, more importantly, how they form their opinions.\n","You will be tasked with identifying and articulating misogyny framings on memes.\n","Misogyny is defined as dislike of, contempt for, or ingrained prejudice against women.'''\n","\n","num_demos=3\n","demos_path='miso_demos.jsonl'\n","demos=list(read_jsonl(demos_path))\n","demos=demos[:num_demos]\n","\n","\n","def add_m(message,base64_image,msg=None,demo=True):\n","  p='''You will be tasked with identifying and articulating misogyny framings on the following memes. You should discuss your reasoning first, and then provide a final decision. Each image provided may or may not contain one or more framings, so your first step is\n","  (a) Reason about whether the image contains a framing (or more framings), or just states something factual or an experience. If the image contains a framing, the next step is \\n(b) Articulate that framing succinctly.\\nYou will perform these steps until the answer to (a) is false, either because there are no framings in the image, or because you have already expressed all the framings.'''\n","\n","  if demo:\n","    message.append({\"role\":\"user\",\"content\":[{\"type\":\"image_url\",\"image_url\":{\"url\": f\"data:image/jpeg;base64,{base64_image}\"}},{\"type\":\"text\",\"text\":p}]})\n","    message.append({\"role\":\"assistant\",\"content\":[{\"type\":\"text\",\"text\":msg}]})\n","  else:\n","     message.append({\"role\":\"user\",\"content\":[{\"type\":\"image_url\",\"image_url\":{\"url\": f\"data:image/jpeg;base64,{base64_image}\"}},{\"type\":\"text\",\"text\":p}]})\n","  return message\n","\n","message=[{\"role\": \"system\", \"content\": sys},]\n","\n","for d in demos:\n","  d_id=d[\"file_name\"]\n","  d_img=os.path.join(demo_image_path, d_id)\n","  base64_image = encode_image(d_img)\n","  message=add_m(message,base64_image,d[\"rationale\"]+d[\"frame\"])\n","\n","message1=message.copy()\n","\n","examples=list(read_jsonl(data_path))\n","\n","for ex in tqdm(examples):\n","  ex_id=ex[\"file_name\"]\n","  if ex_id in seen_ids:\n","    continue\n","  ex_img=os.path.join(image_path, ex_id)\n","  base64_image = encode_image(ex_img)\n","  message=add_m(message,base64_image,demo=False)\n","  #print(message)\n","\n","  completion = chat(\n","                client,\n","                delay=1,\n","                model='gpt-4o',\n","                messages=message,\n","                max_tokens=512,\n","                temperature=1.0,\n","                top_p=0.7,\n","                seed=0,\n","            )\n","\n","  if completion is None:\n","    print(f\"Skipping example due to API safety error: {ex_id}\")\n","    skipped.append(ex_id)\n","    seen_ids.add(ex_id)\n","    continue\n","  content = completion.choices[0].message.content\n","  if content == '':\n","    skipped.append(ex_id)\n","  else:\n","    preds.append({\"id\": ex_id,\"content\": content})\n","  seen_ids.add(ex_id)\n","  message=message1.copy()\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XYLf_9DUqjsE"},"outputs":[],"source":["import re\n","\n","def extract_problems2(reasoning):\n","    p=[]\n","    for r in reasoning:\n","        r=r.lower()\n","        i=r.find(\"of\")\n","        j=r.find(\"arises\")\n","        if i!=-1 and j!=-1:\n","            p.append(r[i+len(\"of\")+1:j].strip())\n","    return p\n","\n","def extract_reason(message):\n","    if message == '':\n","        return []\n","    found_frames = []\n","    message = re.sub(r'\\n\\n', r'\\n', message)\n","    message_list = message.split(\"\\n\") if \"\\n\" in message else message.split(\"\\\\n\")\n","    #reasoning = None\n","    for line in message_list:\n","        line = line.strip()\n","        if not line:\n","            continue\n","        try:\n","            m_id = line.split(\":\")[0]\n","            if m_id is not None:\n","                try:\n","                    mf_id, mt = m_id.split(\".\")\n","                except:\n","                    mt=m_id\n","                    mf_id=1\n","        except:\n","            return []\n","        content = line[len(m_id) + 1 :].strip()\n","        if mt == \"a\":\n","            #reasoning = content\n","            found_frames.append(content)\n","    return found_frames\n","\n","def extract_frames(message):\n","    if message == '':\n","        return []\n","    found_frames = []\n","    message = re.sub(r'\\n\\n', r'\\n', message)\n","    message=re.sub(r'\\\\n', r'\\n', message)\n","    message_list = message.split(\"\\n\") if \"\\n\" in message else message.split(\"\\\\n\")\n","    for line in message_list:\n","        line = line.strip()\n","        if not line:\n","            continue\n","        try:\n","            m_id = line.split(\":\")[0]\n","            if m_id is not None:\n","                try:\n","                    mf_id, mt = m_id.split(\".\")\n","                except:\n","                    mt=m_id\n","                    mf_id=1\n","        except:\n","            return []\n","        content = line[len(m_id) + 1 :].strip()\n","        if mt == \"b\":\n","            found_frames.append(content)\n","    return found_frames"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EXlM8Es455x6"},"outputs":[],"source":["write_jsonl('test_preds.jsonl',preds)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EKY0JFPnCMxa"},"outputs":[],"source":["\n","preds=list(read_jsonl('test_preds.jsonl'))\n","pred_path='test_predictions'\n","os.makedirs(pred_path,exist_ok=True)\n","all_frames=[]\n","annotations = []\n","for j in preds:\n","    local_ann=[]\n","    r=extract_reason(j['content'])\n","    f=extract_frames(j['content'])\n","    for reason,frame in zip(r,f):\n","        all_frames.append({'reasoning':reason,'text':frame})\n","        local_ann.append({'reasoning':reason,'text':frame})\n","    annotations.append({\"articulations\":local_ann,\"id\":j['id']})\n","\n","seen = set()\n","dup_count = defaultdict(int)\n","unique_count=0\n","unique_dict = {}\n","unique_articulations = []\n","for i,f in enumerate(all_frames):\n","    dup_count[f['text']] += 1\n","    if f['text'] in seen:\n","        continue\n","    seen.add(f['text'])\n","    unique_articulations.append(f)\n","    unique_dict[i]=unique_count\n","    unique_count+=1\n","for frame in unique_articulations:\n","    frame['count']=dup_count[frame['text']]\n","\n","write_jsonl(os.path.join(pred_path, \"articulations-full.jsonl\"), all_frames)\n","write_jsonl(os.path.join(pred_path, \"articulations-unique.jsonl\"), unique_articulations,)\n","write_jsonl(os.path.join(pred_path, \"articulation-annotations.jsonl\"), annotations,)"]},{"cell_type":"markdown","metadata":{},"source":["Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LpNCobhwW5rC"},"outputs":[],"source":["import json\n","def build_parent_dict(subproblem_parent_pairs):\n","    \"\"\"Create a dictionary mapping subproblems to their parents.\"\"\"\n","    parent_dict = {}\n","    for subproblem, parent in subproblem_parent_pairs:\n","        parent_dict[subproblem] = parent\n","    return parent_dict\n","\n","def find_root_ancestor(subproblem, parent_dict):\n","    \"\"\"Recursively or iteratively find the root ancestor of a given subproblem.\"\"\"\n","    current = subproblem\n","    while current in parent_dict and parent_dict[current] != current:\n","        current = parent_dict[current]\n","    return current\n","\n","\n","subproblems=list(read_jsonl('subproblems_1c.jsonl'))\n","\n","subproblems_list=[]\n","for subproblem in subproblems:\n","\n","  if 'subproblem(' not in subproblem['response']:\n","    continue\n","  pt=(subproblem['response'].split('subproblem(')[1]).split(')')[0].split(', ')\n","  if len(pt)>1:\n","    subproblems_list.append(tuple(pt))\n","\n","parent_dict = build_parent_dict(subproblems_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"41EVHWESsYgQ"},"outputs":[],"source":["def extract_problems2(r):\n","    r=r.lower()\n","    i=r.find(\"of\")\n","    j=r.find(\"arises\")\n","    if i!=-1 and j!=-1:\n","      return r[i+len(\"of\")+1:j].strip()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tfVhbaSZnxvg"},"outputs":[],"source":["with open('test_labels.txt','r') as f:\n","  all_labels={}\n","  lines=f.readlines()\n","  for line in lines:\n","    a=line.split('\\t')\n","    all_labels[a[0]]=[int(x) for x in a[1:]]\n","  f.close()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QDDAef-uYj0Y"},"outputs":[],"source":["#task b for their annotation\n","from collections import defaultdict\n","import os\n","updated_merged_dict=read_from_jsonl_ind('updated_merged_dict_final.jsonl')\n","labels=['objectification','shaming','stereotyping','violence']\n","labels_map={}\n","labels_dict={}\n","non_miso_list=[]\n","for i in read_jsonl('test_predictions/articulation-annotations.jsonl'):\n","  label=[]\n","  frames=i['articulations']\n","  if len(frames)==0:\n","    non_miso_list.append(i['id'])\n","    continue\n","  for j in frames:\n","    problem=extract_problems2(j['reasoning'])\n","    if problem:\n","      root=find_root_ancestor(problem,parent_dict)\n","\n","      if root not in labels:\n","        label.append('none')\n","      else:\n","        label.append(root)\n","    else:\n","      continue\n","  if len(label)==0:\n","    non_miso_list.append(i['id'])\n","  else:\n","    labels_dict[i['id']]=label\n","\n","\n","\n","\n","#71"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aM6lLAI_D59A"},"outputs":[],"source":["org_non_miso=[]\n","for g in all_labels.keys():\n","  if all_labels[g][0]==0:\n","    org_non_miso.append(g)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V9z4Np_Wtvcl"},"outputs":[],"source":["for key, item in labels_dict.items():\n","  temp=[1,0,0,0,0]\n","  if 'shaming' in item:\n","    temp[1]=1\n","  if 'stereotyping' in item:\n","    temp[2]=1\n","  if 'objectification' in item:\n","    temp[3]=1\n","  if 'violence' in item:\n","    temp[4]=1\n","  labels_dict[key]=temp\n","\n","for id in non_miso_list:\n","  labels_dict[id]=[0,0,0,0,0]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3021,"status":"ok","timestamp":1726259311457,"user":{"displayName":"Rakshitha Rao Ailneni","userId":"10554421985684110379"},"user_tz":300},"id":"TzzFlHaxjMoU","outputId":"5e442697-ab65-4a76-de9d-fbe00f024e50"},"outputs":[],"source":["import pandas as pd\n","from sklearn.metrics import f1_score\n","pred_df=pd.read_csv('pred_df.csv')\n","gold_df=pd.read_csv('gold_df.csv')\n","pred=pred_df[['shaming','stereotyping','objectification','violence']].values.tolist()\n","gold=gold_df[['shaming','stereotyping','objectification','violence']].values.tolist()\n","print(f1_score(gold,pred,average='weighted'))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":156,"status":"ok","timestamp":1726259319130,"user":{"displayName":"Rakshitha Rao Ailneni","userId":"10554421985684110379"},"user_tz":300},"id":"wOWNzWlqspFW","outputId":"d130a08a-704e-40fa-8591-ef0d3e24059d"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.8849396033847943\n"]}],"source":["#task a\n","def check_zeros(lists):\n","    result = []\n","    for lst in lists:\n","        if all(x == 0 for x in lst):\n","            result.append(0)\n","        else:\n","            result.append(1)\n","    return result\n","gold=check_zeros(gold)\n","pred=check_zeros(pred)\n","print(f1_score(gold,pred,average='macro'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5_kpdTrhlZgH"},"outputs":[],"source":["import pandas as pd\n","pred_df=pd.DataFrame(columns=['file_name','shaming','stereotyping','objectification','violence'])\n","i=0\n","for x,f in zip(gold,files):\n","  pred_df.loc[i,'shaming']=x[0]\n","  pred_df.loc[i,'stereotyping']=x[1]\n","  pred_df.loc[i,'objectification']=x[2]\n","  pred_df.loc[i,'violence']=x[3]\n","  pred_df.loc[i,'file_name']=f\n","  i+=1\n","\n","pred_df.to_csv('gold_df.csv')"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"gpt","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":0}
